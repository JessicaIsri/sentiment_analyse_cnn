{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3112ff-2912-4efc-aae7-c4a082adab03",
   "metadata": {},
   "source": [
    "## Importing necessary Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9de64f5-7387-4ef0-9d99-b85126050d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import pad_sequences, to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3cd39-daec-43da-956f-9fc45dbfb0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download(['all-corpora'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc274b3-a3c3-496b-b789-885d15e45e18",
   "metadata": {},
   "source": [
    "## Loading xlsx file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dceebf19-ebbc-4265-a812-a2fbca8e1c79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Caption</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>How I feel today #legday #jelly #aching #gym</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.txt</td>\n",
       "      <td>@ArrivaTW absolute disgrace two carriages from...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.txt</td>\n",
       "      <td>This is my Valentine's from 1 of my nephews. I...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000.txt</td>\n",
       "      <td>betterfeelingfilms: RT via Instagram: First da...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001.txt</td>\n",
       "      <td>Zoe's first love #Rattled @JohnnyHarper15</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File Name                                            Caption     LABEL\n",
       "0     1.txt      How I feel today #legday #jelly #aching #gym   negative\n",
       "1    10.txt  @ArrivaTW absolute disgrace two carriages from...  negative\n",
       "2   100.txt  This is my Valentine's from 1 of my nephews. I...  positive\n",
       "3  1000.txt  betterfeelingfilms: RT via Instagram: First da...   neutral\n",
       "4  1001.txt         Zoe's first love #Rattled @JohnnyHarper15   positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"LabeledText.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf097aa9-7d43-4c04-9bab-03382f397ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4869, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401c1c8-7c29-4f72-84b6-cc36bd1de04f",
   "metadata": {},
   "source": [
    "#### See label distribuition of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c0bc4e-44f5-4b70-a53f-7d34e2efcbb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     36.372972\n",
       "positive    33.805710\n",
       "negative    29.821319\n",
       "Name: LABEL, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['LABEL'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cbab23-8eb4-4a52-92c3-9d929e3e7358",
   "metadata": {},
   "source": [
    "#### Check if exists null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e84df6f-0bab-49c8-8774-fd12d60b51e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File Name    0\n",
       "Caption      0\n",
       "LABEL        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d32eae-3df1-4433-9662-aaf8b30f10f5",
   "metadata": {},
   "source": [
    "## Prepare captions, remove stop words and mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b7cf70b-fb3e-4f2c-860f-9db2943b5b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(input):\n",
    "    # this lib suport portuguese, but my dataset is in english \n",
    "    list_stp = stopwords.words('english')\n",
    "    whitelist = [\"n't\", 'not', 'no']\n",
    "    \n",
    "    # split phrase by word\n",
    "    words  = input.split()\n",
    "    clean = [word for word in words if word not in list_stp or word in whitelist and len(words) > 1]\n",
    "    return ' '.join(clean)\n",
    "\n",
    "remove_mention = lambda text: re.sub(r'@\\w+', '', text)\n",
    "remove_hashtag = lambda text: re.sub('#', '', text)\n",
    "remove_urls = lambda text: re.sub('https?:\\/\\/\\S+', '', text)\n",
    "\n",
    "\n",
    "def clean_text(input_text):\n",
    "    clean = remove_stopwords(input_text)\n",
    "    clean = remove_mention(clean)\n",
    "    clean = remove_hashtag(clean)\n",
    "    clean = remove_urls(clean)\n",
    "    \n",
    "    \n",
    "    return clean\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8661269e-4f34-42d5-95c1-ac741449c158",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove stop_words: @ArrivaTW absolute disgrace two carriages Bangor half way standing room #disgraced\n",
      "remove mentions:  absolute disgrace two carriages from Bangor half way there standing room only #disgraced \n",
      "remove hashtags: @ArrivaTW absolute disgrace two carriages from Bangor half way there standing room only disgraced \n",
      "text clean:  absolute disgrace two carriages Bangor half way standing room disgraced\n"
     ]
    }
   ],
   "source": [
    "sample = df['Caption'][1]\n",
    "\n",
    "clear_text = remove_stopwords(sample)\n",
    "print(f'remove stop_words: {clear_text}')\n",
    "\n",
    "clear_text = remove_mention(sample)\n",
    "print(f'remove mentions: {clear_text}')\n",
    "\n",
    "clear_text = remove_hashtag(sample)\n",
    "print(f'remove hashtags: {clear_text}')\n",
    "\n",
    "clear_text = clean_text(sample)\n",
    "print(f'text clean: {clear_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6de6af-cb8c-4abc-a654-0fdad9a62bf0",
   "metadata": {},
   "source": [
    "### Apply clean into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eac2a96-8ff4-4412-a5c2-e51e7275cfea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Caption</th>\n",
       "      <th>LABEL</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>How I feel today #legday #jelly #aching #gym</td>\n",
       "      <td>negative</td>\n",
       "      <td>How I feel today legday jelly aching gym</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.txt</td>\n",
       "      <td>@ArrivaTW absolute disgrace two carriages from...</td>\n",
       "      <td>negative</td>\n",
       "      <td>absolute disgrace two carriages Bangor half w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.txt</td>\n",
       "      <td>This is my Valentine's from 1 of my nephews. I...</td>\n",
       "      <td>positive</td>\n",
       "      <td>This Valentine's 1 nephews. I elated; sometime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000.txt</td>\n",
       "      <td>betterfeelingfilms: RT via Instagram: First da...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>betterfeelingfilms: RT via Instagram: First da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001.txt</td>\n",
       "      <td>Zoe's first love #Rattled @JohnnyHarper15</td>\n",
       "      <td>positive</td>\n",
       "      <td>Zoe's first love Rattled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  File Name                                            Caption     LABEL  \\\n",
       "0     1.txt      How I feel today #legday #jelly #aching #gym   negative   \n",
       "1    10.txt  @ArrivaTW absolute disgrace two carriages from...  negative   \n",
       "2   100.txt  This is my Valentine's from 1 of my nephews. I...  positive   \n",
       "3  1000.txt  betterfeelingfilms: RT via Instagram: First da...   neutral   \n",
       "4  1001.txt         Zoe's first love #Rattled @JohnnyHarper15   positive   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0           How I feel today legday jelly aching gym  \n",
       "1   absolute disgrace two carriages Bangor half w...  \n",
       "2  This Valentine's 1 nephews. I elated; sometime...  \n",
       "3  betterfeelingfilms: RT via Instagram: First da...  \n",
       "4                          Zoe's first love Rattled   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_text'] = df['Caption'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc863ae-00f4-4265-9dc5-518a6f9c88e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 9.874512220168413)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(df['cleaned_text'].apply(lambda x: len(x.split())),.5), np.mean(df['cleaned_text'].apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a69829-4e4e-4d62-8aaf-3de5a3121ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters in the cleaned text column: 160\n",
      "number of unique words in the cleaned text column: 17125\n",
      "average number of words in the cleaned text column: 10\n",
      "max number of words in the cleaned text column: 28\n"
     ]
    }
   ],
   "source": [
    "unique_chars = set(''.join(df['cleaned_text'].str.lower()))\n",
    "unique_words = set(np.hstack(df['cleaned_text'].str.lower().str.split()))\n",
    "avg_words = round(np.mean(df['cleaned_text'].apply(lambda x: len(x.split()))))\n",
    "max_number_of_words = max(df['cleaned_text'].apply(lambda x: len(x.split())))\n",
    "\n",
    "\n",
    "print(f'number of unique characters in the cleaned text column: {len(unique_chars)}')\n",
    "print(f'number of unique words in the cleaned text column: {len(unique_words)}')\n",
    "print(f'average number of words in the cleaned text column: {avg_words}')\n",
    "print(f'max number of words in the cleaned text column: {max_number_of_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875647c2-7514-4d9d-a01b-91738b26da0b",
   "metadata": {},
   "source": [
    "## Splitting the dataset into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c105468d-b10b-42bf-9121-421f5dc742e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['cleaned_text'], df['LABEL'], test_size=0.2, stratify=df['LABEL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22249416-49be-4ddd-b45e-28fc73804a32",
   "metadata": {},
   "source": [
    "## Tokenizer words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4177bb8b-3112-4318-a40c-11094a077982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "\n",
    "x_train_s = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_s = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1728b13d-4f7e-48a8-a445-13ad6a1f1efc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400    Intrepid 2016 Elite goes 9-2-1 summer season. ...\n",
       "2749    . Dogs R often stunned electrocution take 3 mi...\n",
       "363      new garden year delighted thankyouforvisiting...\n",
       "4063    RT : The new Spectre trailer intense, action-p...\n",
       "132     plastic workshop Loughborough uni pink sad art...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24a62a70-36f0-493b-8e2e-fdaea3a0c1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7345, 1380, 7346, 705, 282, 20, 25, 229, 918, 159, 3927, 1430, 80, 7347],\n",
       " [209, 237, 1196, 365, 10012, 16, 54, 2403, 456, 1618, 10013, 606],\n",
       " [8, 1177, 96, 91, 5543, 5544],\n",
       " [1, 4, 8, 12363, 1240, 137, 1307, 4805, 462, 166],\n",
       " [1354, 5091, 5092, 2980, 767, 254, 127, 5093, 1155, 66]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_s[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a94642-1bcb-4602-a012-9e25e74cc696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intrepid 2016 elite goes 9 2 1 summer season way rep orange blue intrepidfamily',\n",
       " 'dogs r often stunned electrocution take 3 mins others die dragging stopboknal2015',\n",
       " 'new garden year delighted thankyouforvisiting longtailedtit',\n",
       " 'rt the new spectre trailer intense action packed finally here',\n",
       " 'plastic workshop loughborough uni pink sad art juxtaposition bright vibrant']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(x_train_s)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bf767-1cad-4197-bd51-cd088baadcc6",
   "metadata": {},
   "source": [
    "### padding sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2969256c-df9e-4798-9b3c-50bb60b0ced3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = max_number_of_words\n",
    "\n",
    "padded_x_train = pad_sequences(x_train_s, maxlen=max_length, padding='post', truncating='post')\n",
    "padded_x_test = pad_sequences(x_test_s, maxlen=max_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bdb3f-3949-44a9-9af4-79b3b5d673c2",
   "metadata": {},
   "source": [
    "### One hot encoding the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f83c3af-a8bf-4ac3-af28-d3031ae72416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'negative': 0,\n",
    "    'neutral': 1,\n",
    "    'positive': 2\n",
    "}\n",
    "\n",
    "y_train_mapped = y_train.map(mapping)\n",
    "y_test_mapped = y_test.map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "839838f3-a44a-42ab-9de7-5e881770c9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target before  mapping\n",
      "1400    positive\n",
      "2749    negative\n",
      "363     positive\n",
      "4063    negative\n",
      "132     positive\n",
      "Name: LABEL, dtype: object\n",
      "target after mapping\n",
      "1400    2\n",
      "2749    0\n",
      "363     2\n",
      "4063    0\n",
      "132     2\n",
      "Name: LABEL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('target before  mapping')\n",
    "print(y_train[:5])\n",
    "\n",
    "print('target after mapping')\n",
    "print(y_train_mapped[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e28631b8-81c0-4fa6-babe-c5b69b584b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_encoded = to_categorical(y_train_mapped)\n",
    "y_test_encoded = to_categorical(y_test_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa883154-34e4-48a9-9a98-e74518fb7924",
   "metadata": {},
   "source": [
    "## Building a model to classify the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "241d0d6f-a67f-4aea-afb5-ec2e58967e26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 28\n",
      "Vocabulary size: 13826\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(tokenizer.word_index) + 1\n",
    "print(f'Length: {max_length}')\n",
    "print(f'Vocabulary size: {vocab_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee7fd858-58bb-479c-a926-52ff4b045d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import  GlobalMaxPooling1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2e8cfe1-2595-4bea-b2b2-c17997c4f752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential(name='sentiment_classification')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_length, output_dim=300, input_length=max_length))\n",
    "model.add(Conv1D(activation=\"relu\", padding=\"valid\", filters=300, kernel_size=7))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(600, activation='relu'))\n",
    "model.add(Dense(len(set(y_train)), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39695015-7f8f-4c5a-a572-e202cf7053b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 28, 300)           4147800   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 22, 300)           630300    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 300)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 600)               180600    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1803      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,960,503\n",
      "Trainable params: 4,960,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b89fc07c-4dc4-4748-bcbb-4ea501b1d19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 - 8s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 1.3892 - val_accuracy: 0.6923 - 8s/epoch - 74ms/step\n",
      "Epoch 2/10\n",
      "110/110 - 8s - loss: 0.0020 - accuracy: 0.9994 - val_loss: 1.3605 - val_accuracy: 0.6923 - 8s/epoch - 74ms/step\n",
      "Epoch 3/10\n",
      "110/110 - 8s - loss: 0.0013 - accuracy: 0.9994 - val_loss: 1.4019 - val_accuracy: 0.6974 - 8s/epoch - 75ms/step\n",
      "Epoch 4/10\n",
      "110/110 - 8s - loss: 0.0012 - accuracy: 0.9994 - val_loss: 1.4271 - val_accuracy: 0.6846 - 8s/epoch - 74ms/step\n",
      "Epoch 5/10\n",
      "110/110 - 8s - loss: 8.9479e-04 - accuracy: 0.9997 - val_loss: 1.5352 - val_accuracy: 0.6872 - 8s/epoch - 75ms/step\n",
      "Epoch 6/10\n",
      "110/110 - 8s - loss: 8.3639e-04 - accuracy: 0.9997 - val_loss: 1.4776 - val_accuracy: 0.6974 - 8s/epoch - 74ms/step\n",
      "Epoch 7/10\n",
      "110/110 - 8s - loss: 0.0014 - accuracy: 0.9994 - val_loss: 1.4843 - val_accuracy: 0.6821 - 8s/epoch - 74ms/step\n",
      "Epoch 8/10\n",
      "110/110 - 8s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 1.5635 - val_accuracy: 0.6872 - 8s/epoch - 74ms/step\n",
      "Epoch 9/10\n",
      "110/110 - 8s - loss: 0.0011 - accuracy: 0.9994 - val_loss: 1.5864 - val_accuracy: 0.6949 - 8s/epoch - 74ms/step\n",
      "Epoch 10/10\n",
      "110/110 - 8s - loss: 0.0010 - accuracy: 0.9994 - val_loss: 1.5692 - val_accuracy: 0.6897 - 8s/epoch - 75ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1858a6a27c0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_x_train, np.array(y_train_encoded), epochs=10, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c8d1492-e3fa-4ad8-83d5-3e86e42c8e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 5ms/step - loss: 1.5056 - accuracy: 0.6910\n",
      "Loss: 1.5056393146514893\n",
      "Accuracy: 69.1%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(padded_x_test, y_test_encoded)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {round(acc*100,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6dacd4-6fc5-4aa1-b0fe-291bf219c7c2",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3361c36-600d-4ada-a429-a574557cacb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = \"Happy to see you in the morning\"\n",
    "data = pad_sequences(tokenizer.texts_to_sequences([sample]), maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7de4e1c6-0af1-4935-9d3b-18649c637202",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "prediction: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "prediction =  model.predict(data)\n",
    "print(f'prediction: {prediction.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ffcc88bf-0f9d-4467-adc7-c5336685faf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "result = list(mapping.keys())[prediction.argmax(1)[0]]\n",
    "print(f\"Sentiment: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8rc1"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
